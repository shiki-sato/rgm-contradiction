# A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems
This repository has the dataset we developed in:

Shiki Sato, Reina Akama, Jun Suzuki and Kentaro Inui. A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems. In Findings of the Association for Computational Linguistics: ACL 2024, Aug. 2024.

## Build
You can build our test set in `dataset` directory by running `build_dataset.py`.

## Format
Each line of all files in `dataset` contains the dialogue context and the subsequent RGM's contradictory or noncontradictory response:

```
{
    "utterances": A list of utterances comprising this dialogue. The last utterance was generated by RGM. All other utterances were extracted from the existing dialogue corpora.
    "speakers": A list of the names of the speakers (A or B) who emitted utterances in the `utterances`. The i-th utterance in this dialogue `utterances[i]` was emitted by the speaker `speakers[i]`.
    "annotation_target_pair": `utterances[annotation_target_pair[0]]` and `utterances[annotation_target_pair[1]]` are contradictory or noncontradictory.
    "rgm_name": Name of the RGM that generated `utterances[-1]`.
    "contradictory_label_count": Number of three annotators who judged `utterances[annotation_target_pair[0]]` and `utterances[annotation_target_pair[1]]` to be contradictory.
}
```

## Copyrights
They are listed in the directories `contexts/*` and `responses/*`. Please make sure to check all of them before use.

## Citation
If you use this test set, please cite the following:

> Shiki Sato, Reina Akama, Jun Suzuki and Kentaro Inui. A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems. In Findings of the Association for Computational Linguistics: ACL 2024, Aug. 2024.
